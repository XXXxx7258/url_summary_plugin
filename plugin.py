import logging
import re
import aiohttp
import asyncio
import urllib.parse
from bs4 import BeautifulSoup
from typing import List, Tuple, Optional, Set, Type
from collections import OrderedDict
import time
import ipaddress
import socket
from urllib.parse import urlparse, urljoin
from src.plugin_system import (
    BasePlugin, register_plugin, BaseAction,
    ComponentInfo, ActionActivationType, ConfigField, ChatMode
)
from src.plugin_system.apis import message_api, send_api, config_api, emoji_api

logger = logging.getLogger(__name__)

# --------- æœ¬è½®æ¿€æ´»URLå»é‡ ---------
_recently_activated_urls: Set[str] = set()

def should_skip_url_activation(url: str) -> bool:
    """
    å¦‚æœ URL åœ¨æœ¬èŠå¤©è¿›ç¨‹ä¸­å·²æ¿€æ´»è¿‡ä¸€æ¬¡ï¼Œåˆ™è·³è¿‡åç»­æ¿€æ´»ã€‚
    """
    if url in _recently_activated_urls:
        return True
    _recently_activated_urls.add(url)
    return False

try:
    from readability import Document
    readability_available = True
except ImportError:
    readability_available = False

# --------- æ¶ˆæ¯å»é‡ç¼“å­˜å®ç°ï¼ˆæ”¯æŒé…ç½®ï¼‰ ---------
recent_messages = OrderedDict()

def get_cache_ttl():
    try:
        plugin_inst = UrlSummaryPlugin.plugin_instance
        if plugin_inst:
            return plugin_inst.get_config("cache.cache_ttl", 600)
    except Exception:
        pass
    return 600

MAX_CACHE = 500

def is_duplicate_message(msg):
    now = time.time()
    cache_ttl = get_cache_ttl()
    if hasattr(msg, "id") and msg.id:
        key = f"id:{msg.id}"
    elif hasattr(msg, "plain_text") and msg.plain_text:
        key = f"hash:{hash(msg.plain_text)}"
    else:
        key = f"hash:{hash(str(msg))}"
    # æ¸…ç†è¿‡æœŸè®°å½•
    keys_to_del = [k for k, v in recent_messages.items() if now - v > cache_ttl]
    for k in keys_to_del:
        recent_messages.pop(k, None)
    if key in recent_messages:
        return True
    recent_messages[key] = now
    if len(recent_messages) > MAX_CACHE:
        recent_messages.popitem(last=False)
    return False

# --------- URLæ‘˜è¦ç¼“å­˜ ---------
url_summary_cache = OrderedDict()
MAX_URL_CACHE = 500

def get_url_cache_ttl():
    try:
        plugin_inst = UrlSummaryPlugin.plugin_instance
        if plugin_inst:
            return plugin_inst.get_config("cache.url_cache_ttl", 3600)
    except Exception:
        pass
    return 3600

def get_url_summary_from_cache(url):
    now = time.time()
    cache_ttl = get_url_cache_ttl()
    # æ¸…ç†è¿‡æœŸ
    keys_to_del = [k for k, v in url_summary_cache.items() if now - v['time'] > cache_ttl]
    for k in keys_to_del:
        url_summary_cache.pop(k, None)
    if url in url_summary_cache:
        url_summary_cache[url]['time'] = now
        return url_summary_cache[url]['summary']
    return None

def set_url_summary_cache(url, summary):
    now = time.time()
    url_summary_cache[url] = {'summary': summary, 'time': now}
    if len(url_summary_cache) > MAX_URL_CACHE:
        url_summary_cache.popitem(last=False)

class UrlSummaryAction(BaseAction):
    """ç½‘å€æ‘˜è¦Action - æ”¯æŒå…³é”®è¯å’ŒLLMåˆ¤æ–­ï¼Œé¿å…é‡å¤è§¦å‘"""
    action_name = "url_summary"
    action_description = "æ£€æµ‹æ¶ˆæ¯ä¸­çš„çœŸå®ç½‘å€å¹¶å‘é€å†…å®¹æ‘˜è¦"
    focus_activation_type = ActionActivationType.KEYWORD
    normal_activation_type = ActionActivationType.KEYWORD
    activation_keywords = ["http://", "https://", "www.", ".com", ".cn", ".net", ".org"]
    keyword_case_sensitive = False
    mode_enable = ChatMode.ALL
    parallel_action = False

    action_parameters = {"url": "è¦å¤„ç†çš„ç½‘é¡µURL"}
    action_require = [
        "ç”¨æˆ·æ¶ˆæ¯åŒ…å«æœ‰æ•ˆHTTP/HTTPSé“¾æ¥æ—¶ä½¿ç”¨",
        "é“¾æ¥é•¿åº¦å¤§äº7å­—ç¬¦ä¸”åŒ…å«åŸŸåæ—¶ä½¿ç”¨"
    ]
    llm_judge_prompt = "æ˜¯å¦éœ€è¦ç”Ÿæˆç½‘é¡µæ‘˜è¦ï¼Ÿæ¡ä»¶æ˜¯æ¶ˆæ¯åŒ…å«URLä¸”æœªé‡å¤ã€‚"
    associated_types = ["text"]

    DEFAULT_TIMEOUT = 10
    DEFAULT_MAX_LENGTH = 400
    MIN_URL_LENGTH = 7
    DEFAULT_MAX_SUBPAGE = 2
    DEFAULT_SUBPAGE_LENGTH = 200
    BLOCKED_HOSTS = {
        'localhost', '127.0.0.1', '0.0.0.0', '::1', 
        'metadata.google.internal', 'metadata.aws', 
        '169.254.169.254'  # AWS/äº‘æœåŠ¡å…ƒæ•°æ®
    }
    
    BLOCKED_PORTS = {22, 23, 135, 139, 445, 3389}  # SSH, Telnet, SMB, RDPç­‰
    
    ALLOWED_SCHEMES = {'http', 'https'}
    
    MAX_REDIRECTS = 5
    MAX_RESPONSE_SIZE = 10 * 1024 * 1024

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.url_validator = re.compile(
            r'^(?:(?:https?):\/\/)?'  
            r'(?:\S+(?::\S*)?@)?'
            r'(?:(?:[a-zA-Z0-9\u00a1-\uffff-]{1,63}\.?)+'
            r'[a-zA-Z\u00a1-\uffff]{2,})'
            r'(?::\d+)?'
            r'(?:[\/?#][^\s"]*)?'
            r'$', re.IGNORECASE
        )

    def is_private_ip(self, ip_str: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç§æœ‰IPåœ°å€"""
        try:
            ip = ipaddress.ip_address(ip_str)
            return (
                ip.is_private or 
                ip.is_loopback or 
                ip.is_link_local or
                ip.is_multicast or
                ip.is_reserved or
                (ip.version == 4 and str(ip).startswith('0.'))
            )
        except ValueError:
            return False

    def resolve_hostname(self, hostname: str) -> List[str]:
        """è§£æä¸»æœºåè·å–æ‰€æœ‰IPåœ°å€ï¼ˆä½¿ç”¨é…ç½®çš„è¶…æ—¶ï¼‰"""
        try:
            dns_timeout = self.get_config("security.dns_timeout", 5)
            socket.setdefaulttimeout(dns_timeout)
            result = socket.getaddrinfo(hostname, None)
            ips = list(set([r[4][0] for r in result]))
            return ips
        except (socket.gaierror, socket.timeout):
            return []
        finally:
            socket.setdefaulttimeout(None)

    def is_safe_url(self, url: str) -> Tuple[bool, str]:
        """
        éªŒè¯URLæ˜¯å¦å®‰å…¨ï¼ˆä½¿ç”¨é…ç½®ï¼‰
        è¿”å›: (æ˜¯å¦å®‰å…¨, é”™è¯¯ä¿¡æ¯)
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å®‰å…¨æ£€æŸ¥
        if not self.get_config("security.enable_security_check", True):
            return True, ""
        
        try:
            parsed = urlparse(url)
        
            # æ£€æŸ¥åè®®
            allowed_schemes = self.get_config("security.allowed_schemes", ["http", "https"])
            if parsed.scheme not in allowed_schemes:
                return False, f"ä¸æ”¯æŒçš„åè®®: {parsed.scheme}"
        
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»æœºå
            if not parsed.hostname:
                return False, "æ— æ•ˆçš„ä¸»æœºå"
        
            hostname = parsed.hostname.lower()
        
            # æ£€æŸ¥é»‘åå•ä¸»æœº
            blocked_hosts = self.get_config("security.blocked_hosts", list(self.BLOCKED_HOSTS))
            if hostname in blocked_hosts:
                return False, f"ç¦æ­¢è®¿é—®çš„ä¸»æœº: {hostname}"
        
            # æ£€æŸ¥ç«¯å£
            port = parsed.port or (443 if parsed.scheme == 'https' else 80)
            blocked_ports = self.get_config("security.blocked_ports", list(self.BLOCKED_PORTS))
            if port in blocked_ports:
                return False, f"ç¦æ­¢è®¿é—®çš„ç«¯å£: {port}"
        
            # æ£€æŸ¥æ˜¯å¦ä¸ºIPåœ°å€
            is_ip = False
            try:
                ipaddress.ip_address(hostname)
                is_ip = True
            except ValueError:
                pass
        
            if is_ip:
                # ç›´æ¥æ˜¯IPåœ°å€ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç§æœ‰IP
                if not self.get_config("security.allow_private_ip", False) and self.is_private_ip(hostname):
                    return False, f"ç¦æ­¢è®¿é—®å†…ç½‘IP: {hostname}"
            else:
                # æ˜¯åŸŸåï¼Œè§£æåæ£€æŸ¥æ‰€æœ‰IP
                if self.get_config("security.enable_dns_check", True):
                    ips = self.resolve_hostname(hostname)
                    if not ips:
                        return False, f"æ— æ³•è§£æåŸŸå: {hostname}"
                
                    if not self.get_config("security.allow_private_ip", False):
                        for ip in ips:
                            if self.is_private_ip(ip):
                                return False, f"åŸŸåè§£æåˆ°å†…ç½‘IP: {hostname} -> {ip}"
        
            #  æ£€æŸ¥URLé•¿åº¦
            max_url_length = self.get_config("security.max_url_length", 2048)
            if len(url) > max_url_length:
                return False, "URLè¿‡é•¿"
        
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å±é™©å­—ç¬¦
            dangerous_patterns = [
                r'\.\./', r'\.\.\\',  
                r'<script', r'javascript:',  
                r'file://', r'gopher://', r'dict://',  
            ]
            for pattern in dangerous_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return False, f"URLåŒ…å«å±é™©æ¨¡å¼"
        
            return True, ""
        
        except Exception as e:
            logger.warning(f"URLå®‰å…¨æ£€æŸ¥å¼‚å¸¸: {str(e)}")
            return False, f"URLéªŒè¯å¤±è´¥: {str(e)}"

    def normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ–URLå¹¶è¿›è¡ŒåŸºç¡€æ¸…ç†"""
        url = urllib.parse.unquote(url.strip())
        if not url:
            return ""
        
        # ç§»é™¤å±é™©å­—ç¬¦
        url = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', url)
        
        # æ·»åŠ åè®®
        if not url.startswith(('http://', 'https://', 'ftp://')):
            url = "https://" + url.lstrip('/')
        
        # åªä¿ç•™http/https
        parsed = urlparse(url)
        if parsed.scheme not in self.ALLOWED_SCHEMES:
            url = "https://" + parsed.netloc + parsed.path
            if parsed.query:
                url += "?" + parsed.query
        
        return url

    async def execute(self) -> Tuple[bool, str]:
        try:
            # æ¿€æ´»å‰æ£€æŸ¥ï¼šåŒè½®æ¬¡å·²å¤„ç†åˆ™è·³è¿‡
            urls = []
            if hasattr(self, 'action_data') and self.action_data.get("url"):
                urls = [self.normalize_url(self.action_data.get("url"))]
            elif hasattr(self, 'message') and self.message:
                urls = self.extract_and_validate_urls(self.message.plain_text)
            if urls and should_skip_url_activation(urls[0]):
                logger.info(f"URL å·²åœ¨æœ¬è½®æ¿€æ´»è¿‡ï¼Œè·³è¿‡æ‰§è¡Œ: {urls[0]}")
                return False, "è¯¥é“¾æ¥å·²å¤„ç†è¿‡"

            # æ¶ˆæ¯å±‚å»é‡
            if hasattr(self, 'message') and is_duplicate_message(self.message):
                logger.info("æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                return False, "å·²å¿½ç•¥é‡å¤æ¶ˆæ¯"

            # æå–URL
            urls = []
            if hasattr(self, 'action_data') and self.action_data.get("url"):
                urls = [self.normalize_url(self.action_data.get("url"))]
            elif hasattr(self, 'message') and self.message:
                urls = self.extract_and_validate_urls(self.message.plain_text)
            elif hasattr(self, 'raw_message') and self.raw_message:
                urls = self.extract_and_validate_urls(self.raw_message)
            if not urls:
                return False, "æœªæ£€æµ‹åˆ°æœ‰æ•ˆURL"
            url = urls[0]

            is_safe, error_msg = self.is_safe_url(url)
            if not is_safe:
                logger.warning(f"URLå®‰å…¨éªŒè¯å¤±è´¥: {url} - {error_msg}")
                await self.send_text(f"âš ï¸ æ— æ³•è®¿é—®è¯¥é“¾æ¥: {error_msg}")
                return False, f"URLä¸å®‰å…¨: {error_msg}"

            # æ£€æŸ¥æ‘˜è¦ç¼“å­˜
            cached = get_url_summary_from_cache(url)
            if cached:
                await self.send_summary(url, cached)
                return True, f"å·²å‘é€ {url} çš„ç¼“å­˜æ‘˜è¦"

            # é…ç½®
            timeout = self.get_config("http.timeout", self.DEFAULT_TIMEOUT)
            max_length = self.get_config("processing.max_length", self.DEFAULT_MAX_LENGTH)
            user_agent = self.get_config("http.user_agent", "Mozilla/5.0")
            max_subpage = self.get_config("processing.max_subpage", self.DEFAULT_MAX_SUBPAGE)
            subpage_length = self.get_config("processing.subpage_length", self.DEFAULT_SUBPAGE_LENGTH)
            enable_related = self.get_config("processing.enable_related_pages", True)

            await self.send_processing_feedback()
            seen = {url}
            summary = await self.get_url_summary(
                url, timeout, max_length, user_agent,
                fetch_links=enable_related, seen_links=seen,
                max_subpage=max_subpage, subpage_length=subpage_length
            )
            if summary:
                set_url_summary_cache(url, summary)
                await self.send_summary(url, summary)
                return True, f"å·²å‘é€ {url} çš„å†…å®¹æ‘˜è¦"
            return False, "æ— æ³•è·å–ç½‘é¡µå†…å®¹"
        except asyncio.TimeoutError:
            logger.warning("è¯·æ±‚è¶…æ—¶")
            await self.send_timeout_message()
            return False, "è¯·æ±‚è¶…æ—¶"
        except Exception as e:
            logger.exception("ç½‘å€æ‘˜è¦å¤„ç†å¤±è´¥")
            await self.send_error_message()
            return False, f"å¤„ç†å¤±è´¥: {str(e)}"

    async def send_processing_feedback(self):
        try:
            emoji_result = await emoji_api.get_by_emotion("processing")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€å¤„ç†è¡¨æƒ…å¤±è´¥: {str(e)}")

    async def send_summary(self, url: str, summary: str):
        display_url = url if len(url) <= 50 else f"{url[:30]}...{url[-20:]}"
        summary_msg = self.format_summary_message(display_url, summary)
        await self.send_text(summary_msg)
        try:
            emoji_result = await emoji_api.get_by_emotion("success")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€æˆåŠŸè¡¨æƒ…å¤±è´¥: {str(e)}")

    def format_summary_message(self, display_url: str, summary: str) -> str:
        parts = summary.split('\n\nç›¸å…³é¡µé¢ï¼š', 1)
        main = parts[0].strip()
        related = parts[1].strip() if len(parts) == 2 else None
        main_str = main.replace(chr(10), '\n> ')
        msg = f"ğŸ”— **ç½‘é¡µæ‘˜è¦** [`{display_url}`]\n\n> {main_str}"
        if related:
            msg += "\n\n<details><summary>ç›¸å…³é¡µé¢</summary>\n\n"
            for sub in re.split(r"\nã€(https?://[^ã€‘]+)ã€‘\n", "\n"+related):
                if not sub.strip():
                    continue
                if sub.startswith("http"):
                    continue
                sub_str = sub.strip().replace(chr(10), '\n> ')
                msg += f"> {sub_str}\n"
            msg += "</details>"
        return msg

    async def send_timeout_message(self):
        try:
            await self.send_text("â±â±â± ç½‘é¡µåŠ è½½è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•")
            emoji_result = await emoji_api.get_by_emotion("timeout")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€è¶…æ—¶æ¶ˆæ¯å¤±è´¥: {str(e)}")

    async def send_error_message(self):
        try:
            await self.send_text("âŒâŒ å¤„ç†ç½‘é¡µå†…å®¹æ—¶å‡ºé”™ï¼Œè¯·ç¨åå†è¯•")
            emoji_result = await emoji_api.get_by_emotion("error")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {str(e)}")

    def extract_and_validate_urls(self, text: str) -> List[str]:
        url_pattern = r'((?:https?://)?(?:www\.)?(?:[a-zA-Z0-9\-]+\.)+[a-zA-Z]{2,}(?:/[^\s<>"]*)?)'
        potential_urls = re.findall(url_pattern, text)
        valid_urls = []
        for url in potential_urls:
            normalized_url = self.normalize_url(url)
            if self.is_valid_url(normalized_url):
                valid_urls.append(normalized_url)
        return list(OrderedDict.fromkeys(valid_urls))

    def is_valid_url(self, url: str) -> bool:
        if len(url) < self.MIN_URL_LENGTH:
            return False
        if '.' not in url:
            return False
        return True

    def sanitize_header_value(self, value: str) -> str:
        """æ¸…ç†è¯·æ±‚å¤´å€¼ï¼Œé˜²æ­¢CRLFæ³¨å…¥"""
        return re.sub(r'[\r\n]', '', str(value))

    async def get_url_summary(
        self,
        url: str,
        timeout: int,
        max_length: int,
        user_agent: str,
        fetch_links: bool = True,
        seen_links: Optional[Set[str]] = None,
        max_subpage: int = 2,
        subpage_length: int = 200
    ) -> Optional[str]:
        is_safe, error_msg = self.is_safe_url(url)
        if not is_safe:
            return f"âš ï¸ {error_msg}"

        headers = {
            "User-Agent": self.sanitize_header_value(user_agent),
            "Accept": self.sanitize_header_value("text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"),
            "Accept-Language": self.sanitize_header_value("zh-CN,zh;q=0.9"),
            "Accept-Encoding": self.sanitize_header_value("gzip, deflate"),
            "DNT": self.sanitize_header_value("1"),
            "Upgrade-Insecure-Requests": self.sanitize_header_value("1")
        }
        proxy_url = self.get_config("http.proxy", "")
        if seen_links is None:
            seen_links = set()

        for attempt in range(3):
            try:
                # åˆ›å»ºè‡ªå®šä¹‰çš„TCPConnectorä»¥æ§åˆ¶DNSè§£æ
                connector = aiohttp.TCPConnector(
                    ssl=False,
                    limit=10,  # è¿æ¥æ± é™åˆ¶
                    ttl_dns_cache=300,  # DNSç¼“å­˜5åˆ†é’Ÿ
                )
                
                timeout_config = aiohttp.ClientTimeout(
                    total=timeout,
                    connect=5,  # è¿æ¥è¶…æ—¶
                    sock_read=timeout
                )

                async with aiohttp.ClientSession(connector=connector) as session:
                    logger.debug(f"å°è¯•è·å–URLå†…å®¹: {url} (å°è¯• {attempt+1}/3), ä»£ç†: {proxy_url}")
                    async with session.get(
                        url,
                        headers=headers,
                        timeout=timeout_config,
                        allow_redirects=False,
                        proxy=proxy_url if proxy_url else None,
                        max_redirects=0  # å®Œå…¨ç¦ç”¨è‡ªåŠ¨é‡å®šå‘
                    ) as response:
                        # å¤„ç†é‡å®šå‘
                        redirect_count = 0
                        current_url = url
                        
                        while response.status in [301, 302, 303, 307, 308]:
                            if redirect_count >= self.MAX_REDIRECTS:
                                return "âš ï¸ é‡å®šå‘æ¬¡æ•°è¿‡å¤š"
                            
                            location = response.headers.get('Location')
                            if not location:
                                break
                            
                            # å¤„ç†ç›¸å¯¹URL
                            new_url = urljoin(current_url, location)
                            
                            # éªŒè¯é‡å®šå‘ç›®æ ‡çš„å®‰å…¨æ€§
                            is_safe, error_msg = self.is_safe_url(new_url)
                            if not is_safe:
                                return f"âš ï¸ ä¸å®‰å…¨çš„é‡å®šå‘: {error_msg}"
                            
                            current_url = new_url
                            redirect_count += 1
                            
                            # é‡æ–°è¯·æ±‚
                            response.close()
                            response = await session.get(
                                current_url,
                                headers=headers,
                                timeout=timeout_config,
                                allow_redirects=False,
                                proxy=proxy_url if proxy_url else None
                            )
                        
                        if response.status != 200:
                            return f"âš ï¸ æ— æ³•è®¿é—®ç½‘é¡µ (çŠ¶æ€ç : {response.status})"
                        
                        
                        content_type = response.headers.get('Content-Type', '').lower()
                        if not any(ct in content_type for ct in ['text/html', 'application/xhtml']):
                            return f"âš ï¸ ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {content_type}"
                        
                        
                        content = b''
                        async for chunk in response.content.iter_chunked(8192):
                            content += chunk
                            if len(content) > self.MAX_RESPONSE_SIZE:
                                return "âš ï¸ ç½‘é¡µå†…å®¹è¿‡å¤§"
                        
                        # æ£€æµ‹ç¼–ç 
                        encoding = response.charset
                        if not encoding:
                            try:
                                import chardet
                                detected = chardet.detect(content)
                                encoding = detected.get('encoding', 'utf-8')
                            except Exception:
                                encoding = 'utf-8'
                        
                        try:
                            html = content.decode(encoding or 'utf-8', errors='ignore')
                        except Exception:
                            html = content.decode('utf-8', errors='ignore')
                                               
                        html = self.sanitize_html(html)
     
                        soup_for_links = BeautifulSoup(html, 'html.parser')
                        soup = BeautifulSoup(html, 'html.parser')
                        summary_mode = self.get_config("processing.summary_mode", "sentence")
                        content = self.extract_main_content(soup, html=html)
                        if summary_mode == "llm" and content:
                            summary = await self.summarize_by_llm(content, max_length)
                            meta_desc = soup.find("meta", attrs={"name": "description"}) or \
                                soup.find("meta", attrs={"property": "og:description"})
                            og_title = soup.find("meta", attrs={"property": "og:title"})
                            og_site = soup.find("meta", attrs={"property": "og:site_name"})
                            title = og_title.get("content", "").strip() if og_title else (soup.title.get_text(strip=True) if soup.title else "")
                            site = og_site.get("content", "").strip() if og_site else ""
                            desc = meta_desc.get("content", "").strip() if meta_desc else ""
                            lines = []
                            if title: lines.append(f"**{title}**")
                            if site: lines.append(f"ï¼ˆ{site}ï¼‰")
                            lines.append(summary)
                            summary = "\n".join(lines).strip()
                        else:
                            summary = self.extract_summary_from_soup(soup, html, max_length)
                        # ç›¸å…³é¡µé¢é€»è¾‘
                        if fetch_links:
                            internal_links = self.extract_internal_links(
                                soup_for_links, url, max_links=max_subpage, seen_links=seen_links
                            )
                            if internal_links:
                                for link in internal_links:
                                    seen_links.add(link)
                                related = await self.get_multi_url_summaries(
                                    internal_links, timeout, subpage_length, user_agent, seen_links=seen_links
                                )
                                if related:
                                    summary += "\n\nç›¸å…³é¡µé¢ï¼š"
                                    for link, sub_summary in related:
                                        link_disp = link if len(link) <= 50 else f"{link[:30]}...{link[-20:]}"
                                        summary += f"\nã€{link_disp}ã€‘\n{sub_summary}"
                        return summary
            except asyncio.TimeoutError:
                logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt+1}/3)")
                if attempt == 2:
                    return "â±ï¸ è¯·æ±‚è¶…æ—¶"
                await asyncio.sleep(1)
            except aiohttp.ClientError as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/3): {type(e).__name__}")
                if attempt == 2:
                    return f"âŒ è¯·æ±‚å¤±è´¥: {type(e).__name__}"
                await asyncio.sleep(1)
            except Exception as e:
                logger.exception("å¤„ç†é”™è¯¯")
                return f"âŒ å¤„ç†é”™è¯¯: {type(e).__name__}"
            finally:
                if 'connector' in locals():
                    await connector.close()
        
        return "âŒ å¤šæ¬¡å°è¯•åä»æ— æ³•è·å–å†…å®¹"

    def sanitize_html(self, html: str) -> str:
        """æ¸…ç†HTMLä¸­çš„å±é™©å†…å®¹"""
        # ç§»é™¤scriptæ ‡ç­¾
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.IGNORECASE | re.DOTALL)
        # ç§»é™¤styleæ ‡ç­¾
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.IGNORECASE | re.DOTALL)
        # ç§»é™¤iframe
        html = re.sub(r'<iframe[^>]*>.*?</iframe>', '', html, flags=re.IGNORECASE | re.DOTALL)
        # ç§»é™¤äº‹ä»¶å¤„ç†å™¨
        html = re.sub(r'\son\w+\s*=\s*["\'][^"\']*["\']', '', html, flags=re.IGNORECASE)
        # ç§»é™¤javascript:é“¾æ¥
        html = re.sub(r'javascript:', '', html, flags=re.IGNORECASE)
        
        return html

    async def get_multi_url_summaries(
        self, urls: list, timeout: int, max_length: int, user_agent: str, seen_links: Optional[Set[str]] = None
    ) -> list:
        """æ‰¹é‡è·å–URLæ‘˜è¦æ—¶è¿›è¡Œå®‰å…¨éªŒè¯"""
        results = []
        if seen_links is None:
            seen_links = set()
            
        for url in urls:
            if url in seen_links:
                continue
                
            # éªŒè¯æ¯ä¸ªURLçš„å®‰å…¨æ€§
            is_safe, error_msg = self.is_safe_url(url)
            if not is_safe:
                logger.warning(f"è·³è¿‡ä¸å®‰å…¨çš„å­é¡µé¢: {url} - {error_msg}")
                continue
                
            cached_summary = get_url_summary_from_cache(url)
            if cached_summary:
                results.append((url, cached_summary))
                seen_links.add(url)
                continue
                
            try:
                summary = await self.get_url_summary(
                    url, timeout, max_length, user_agent, fetch_links=False, seen_links=seen_links
                )
                if summary and not summary.startswith("âš ï¸") and not summary.startswith("âŒ"):
                    set_url_summary_cache(url, summary)
                    results.append((url, summary))
                seen_links.add(url)
            except Exception as e:
                logger.warning(f"å­é¡µé¢æ‘˜è¦æŠ“å–å¤±è´¥: {url}, {str(e)}")
                
        return results

    def extract_internal_links(
        self, soup: BeautifulSoup, base_url: str, max_links: int = 2, seen_links: Optional[Set[str]] = None
    ) -> list:
        """æå–å†…éƒ¨é“¾æ¥æ—¶ä¹Ÿè¦è¿›è¡Œå®‰å…¨éªŒè¯"""
        from urllib.parse import urljoin, urlparse
        base_domain = urlparse(base_url).netloc
        links = []
        seen = set() if seen_links is None else set(seen_links)
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            abs_url = urljoin(base_url, href)
            
            # éªŒè¯é“¾æ¥å®‰å…¨æ€§
            is_safe, _ = self.is_safe_url(abs_url)
            if not is_safe:
                continue
                
            link_domain = urlparse(abs_url).netloc
            
            if not abs_url.startswith(('http://', 'https://')):
                continue
                
            if (
                link_domain == base_domain
                and abs_url not in seen
                and abs_url != base_url
                and not abs_url.startswith('javascript:')
                and not abs_url.startswith('mailto:')
                and not abs_url.endswith(('.jpg', '.png', '.gif', '.svg', '.ico', '.pdf', '.zip', '.exe'))
            ):
                seen.add(abs_url)
                links.append(abs_url)
                
            if len(links) >= max_links:
                break
                
        return links

    def extract_main_content(self, soup: BeautifulSoup, html: str = None) -> str:
        if readability_available and html is not None:
            try:
                doc = Document(html)
                content = doc.summary()
                soup2 = BeautifulSoup(content, 'html.parser')
                text = soup2.get_text(" ", strip=True)
                if len(text) > 50:
                    logger.debug(f"readabilityæ­£æ–‡é•¿åº¦: {len(text)}")
                    return text
            except Exception as e:
                logger.warning(f"readabilityæŠ½å–æ­£æ–‡å¤±è´¥: {str(e)}")
        for tag in ['article', 'main', 'content', 'entry-content', 'body', 'section']:
            element = soup.find(tag)
            if element:
                text = element.get_text(" ", strip=True)
                logger.debug(f"æ ‡ç­¾<{tag}>æ­£æ–‡é•¿åº¦: {len(text)}")
                if len(text) > 50:
                    return text
        for class_name in [
            'wp_articlecontent', 'article-content', 'articleBody', 'article', 'main', 'content', 'entry-content',
            'body', 'post', 'post-content', 'main-content', 'TRS_Editor',
            'news-content', 'content-main', 'articleText', 'contentArea'
        ]:
            element = soup.find(class_=class_name)
            if element:
                text = element.get_text(" ", strip=True)
                logger.debug(f"class={class_name} æ­£æ–‡é•¿åº¦: {len(text)}")
                if len(text) > 50:
                    return text
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text(" ", strip=True)
            if len(text) > 10:
                paragraphs.append(text)
        if paragraphs:
            logger.debug(f"æŠ“åˆ°æ®µè½æ•°: {len(paragraphs)}ï¼Œåˆå¹¶å‰3æ®µè½ä¸ºï¼š{' | '.join(paragraphs[:3])}")
            return " ".join(paragraphs[:20])
        body = soup.body
        if body:
            text = body.get_text(" ", strip=True)
            logger.debug(f"<body>é•¿åº¦: {len(text)}")
            if len(text) > 50:
                return text
        a_tags = soup.find_all('a', href=True)
        headlines = []
        for a in a_tags:
            txt = a.get_text(strip=True)
            if txt and 5 < len(txt) < 40 and not re.search(r"[ã€Šã€‹]", txt):
                headlines.append(txt)
            if len(headlines) >= 5:
                break
        if headlines:
            logger.debug(f"headlines fallback: {' / '.join(headlines)}")
            return " / ".join(headlines)
        logger.debug("æ­£æ–‡æå–å…¨éƒ¨å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
        return ""

    def extract_main_content_html(self, html: str) -> Optional[str]:
        if readability_available and html is not None:
            try:
                doc = Document(html)
                return doc.summary(html_partial=True)
            except Exception as e:
                logger.warning(f"readabilityæ­£æ–‡htmlæŠ½å–å¤±è´¥: {str(e)}")
        return None

    def summarize_text(self, text: str, max_length: int = 400) -> str:
        import re
        text = re.sub(r'([a-zA-Z0-9])ã€‚([a-zA-Z0-9])', r'\1.\2', text)
        text = text.strip()
        if not text:
            return ""
        if len(text) <= max_length:
            return text

        summary_mode = "sentence"
        try:
            summary_mode = self.get_config("processing.summary_mode", "sentence")
        except Exception:
            pass

        if summary_mode == "llm":
            return "[LLMæ‘˜è¦å¤„ç†ä¸­...]"

        if summary_mode == "plain":
            trunc_point = -1
            for sep in ['ã€‚', 'ï¼', '!', 'ï¼Ÿ', '?', '.', '\n']:
                idx = text.rfind(sep, 0, max_length)
                if idx > trunc_point:
                    trunc_point = idx
            if trunc_point != -1 and trunc_point > max_length // 3:
                return text[:trunc_point + 1] + "..."
            return text[:max_length] + "..."

        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ!?\.])', text)
        result = ''
        total = 0
        for i in range(0, len(sentences) - 1, 2):
            seg = sentences[i] + sentences[i + 1]
            if total + len(seg) > max_length:
                break
            result += seg
            total += len(seg)
        if not result:
            return text[:max_length] + "..."
        return result.strip() + "..."

    async def summarize_by_llm(self, text, max_length: int) -> str:
        try:
            from src.plugin_system.apis import llm_api
            logger.info(f"[LLMæ‘˜è¦è°ƒç”¨] promptå‰100å­—: {text[:100]}")
            model_config_obj = llm_api.get_available_models()
            logger.info(f"modelsè·å–ç»“æœ: {model_config_obj}, ç±»å‹: {type(model_config_obj)}")
            model_config_key = self.get_config("processing.llm_config_key", "utils_small")
            model_config = getattr(model_config_obj, model_config_key, None)
            if not model_config:
                logger.warning(f"æœªæ‰¾åˆ°æŒ‡å®šæ¨¡å‹ {model_config_key}ï¼Œå°è¯• fallback")
                model_config = getattr(model_config_obj, "replyer_1", None)
            if not model_config:
                logger.error("æœªè·å–åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹é…ç½®ï¼Œé™çº§æœ¬åœ°æ‘˜è¦")
                return self.summarize_text(text, max_length)
            prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹å‹ç¼©ä¸ºä¸è¶…è¿‡{max_length}å­—çš„ä¸­æ–‡æ‘˜è¦ï¼š\n{text}"
            success, response, reasoning, model_used = await llm_api.generate_with_model(prompt, model_config)
            logger.info(f"[LLMæ‘˜è¦è°ƒç”¨] model={model_used}, success={success}, responseå‰100å­—={response[:100] if response else response}")
            if success and response:
                return response.strip()
            else:
                logger.error("[LLMæ‘˜è¦è°ƒç”¨] LLMæœªè¿”å›ç»“æœï¼Œå›é€€æœ¬åœ°æ‘˜è¦")
                return self.summarize_text(text, max_length)
        except Exception as e:
            logger.exception(f"[LLMæ‘˜è¦è°ƒç”¨] å¤±è´¥: {e}")
            return self.summarize_text(text, max_length)       

    def extract_summary_from_soup(self, soup: BeautifulSoup, html: str, max_length: int) -> str:
        meta_desc = soup.find("meta", attrs={"name": "description"}) or \
            soup.find("meta", attrs={"property": "og:description"})
        og_title = soup.find("meta", attrs={"property": "og:title"})
        og_site = soup.find("meta", attrs={"property": "og:site_name"})
        title = og_title.get("content", "").strip() if og_title else (soup.title.get_text(strip=True) if soup.title else "")
        site = og_site.get("content", "").strip() if og_site else ""
        desc = meta_desc.get("content", "").strip() if meta_desc else ""
        content = self.extract_main_content(soup, html=html)
        summary_mode = "sentence"
        try:
            summary_mode = self.get_config("processing.summary_mode", "sentence")
            logger.info(f"[æ‘˜è¦æµç¨‹] summary_mode={summary_mode}")
        except Exception:
            pass
        logger.info(f"[æ‘˜è¦æµç¨‹] summary_mode={summary_mode}, contentå‰50å­—: {content[:50]}")
        if summary_mode == "llm" and content:
            summary = "[[LLMæ‘˜è¦å¤„ç†ä¸­]]"
        else:
            summary = desc if desc else self.summarize_text(content, max_length)
        lines = []
        if title: lines.append(f"**{title}**")
        if site: lines.append(f"ï¼ˆ{site}ï¼‰")
        lines.append(summary)
        return "\n".join(lines).strip()

@register_plugin
class UrlSummaryPlugin(BasePlugin):
    plugin_name = "url_summary_plugin"
    plugin_description = "è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„çœŸå®ç½‘å€å¹¶å‘é€å†…å®¹æ‘˜è¦ï¼ˆåŒ…æ‹¬é‡è¦å†…é“¾ï¼‰- å¢å¼ºå®‰å…¨ç‰ˆæœ¬"
    plugin_version = "2.4.0"  # æ›´æ–°ç‰ˆæœ¬å·
    plugin_author = "qingkong"
    dependencies = []
    python_dependencies = []
    enable_plugin = True
    config_file_name = "config.toml"
    config_section_descriptions = {
        "general": "é€šç”¨è®¾ç½®",
        "http": "HTTPè¯·æ±‚è®¾ç½®",
        "processing": "å†…å®¹å¤„ç†è®¾ç½®",
        "cache": "ç¼“å­˜è®¾ç½®",
        "security": "å®‰å…¨è®¾ç½®"  # æ–°å¢å®‰å…¨é…ç½®éƒ¨åˆ†
    }
    config_schema = {
        "config_version": ConfigField(type=str, default="1.0.0", description="é…ç½®ç‰ˆæœ¬"),
        "general": {
            "enabled": ConfigField(type=bool, default=True, description="æ˜¯å¦å¯ç”¨æ’ä»¶"),
            "enable_group": ConfigField(type=bool, default=True, description="æ˜¯å¦åœ¨ç¾¤èŠå¯ç”¨"),
            "enable_private": ConfigField(type=bool, default=True, description="æ˜¯å¦åœ¨ç§èŠå¯ç”¨")
        },
        "http": {
            "timeout": ConfigField(type=int, default=10, description="è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"),
            "user_agent": ConfigField(
                type=str,
                default="Mozilla/5.0 (compatible; MaiBot-URL-Summary/2.0)",
                description="HTTPè¯·æ±‚ä½¿ç”¨çš„User-Agent"
            ),
            "max_retries": ConfigField(type=int, default=3, description="æœ€å¤§é‡è¯•æ¬¡æ•°"),
            "proxy": ConfigField(type=str, default="", description="HTTPè¯·æ±‚æ‰€ç”¨çš„ä»£ç†åœ°å€ï¼Œå¦‚ http://127.0.0.1:7890"),
            "max_redirects": ConfigField(type=int, default=5, description="æœ€å¤§é‡å®šå‘æ¬¡æ•°"),
            "max_response_size": ConfigField(type=int, default=10485760, description="æœ€å¤§å“åº”å¤§å°(å­—èŠ‚)ï¼Œé»˜è®¤10MB")
        },
        "processing": {
            "max_length": ConfigField(type=int, default=400, description="æ‘˜è¦æœ€å¤§é•¿åº¦"),
            "include_title": ConfigField(type=bool, default=True, description="æ˜¯å¦åŒ…å«æ ‡é¢˜"),
            "min_content_length": ConfigField(type=int, default=100, description="æœ€å°å†…å®¹é•¿åº¦"),
            "max_subpage": ConfigField(type=int, default=2, description="ç›¸å…³é¡µé¢æœ€å¤šæŠ“å–æ•°é‡"),
            "subpage_length": ConfigField(type=int, default=200, description="ç›¸å…³é¡µé¢æ‘˜è¦æœ€å¤§é•¿åº¦"),
            "enable_related_pages": ConfigField(type=bool, default=True, description="æ˜¯å¦æŠ“å–ç«™å†…ç›¸å…³é¡µé¢æ‘˜è¦"),
            "summary_mode": ConfigField(
                type=str,
                default="sentence",
                description="æ‘˜è¦ç”Ÿæˆæ–¹å¼ï¼Œå¯é€‰ llmï¼ˆæ™ºèƒ½æ‘˜è¦ï¼‰ã€sentenceï¼ˆæŒ‰å¥æˆªæ–­ï¼‰ã€plainï¼ˆåŸæ ·æˆªæ–­ï¼‰"
            ),
            "llm_config_key": ConfigField(
                type=str,
                default="utils_small",
                description="LLMæ‘˜è¦æ—¶é‡‡ç”¨çš„æ¨¡å‹é…ç½®keyï¼Œä¾‹å¦‚ï¼šutils_small, replyer_1, replyer_2"
            )
        },
        "cache": {
            "cache_ttl": ConfigField(type=int, default=600, description="é˜²é‡å¤ç¼“å­˜æ—¶é—´(ç§’)"),
            "url_cache_ttl": ConfigField(type=int, default=3600, description="URLæ‘˜è¦ç¼“å­˜æ—¶é—´(ç§’)")
        },
        "security": {  # æ–°å¢å®‰å…¨é…ç½®
            "enable_security_check": ConfigField(type=bool, default=True, description="æ˜¯å¦å¯ç”¨å®‰å…¨æ£€æŸ¥"),
            "allow_private_ip": ConfigField(type=bool, default=False, description="æ˜¯å¦å…è®¸è®¿é—®å†…ç½‘IP"),
            "blocked_hosts": ConfigField(
                type=list, 
                default=["localhost", "127.0.0.1", "0.0.0.0", "::1", "169.254.169.254"],
                description="ç¦æ­¢è®¿é—®çš„ä¸»æœºåˆ—è¡¨"
            ),
            "blocked_ports": ConfigField(
                type=list,
                default=[22, 23, 135, 139, 445, 3389],
                description="ç¦æ­¢è®¿é—®çš„ç«¯å£åˆ—è¡¨"
            ),
            "allowed_schemes": ConfigField(
                type=list,
                default=["http", "https"],
                description="å…è®¸çš„URLåè®®"
            ),
            "max_url_length": ConfigField(type=int, default=2048, description="æœ€å¤§URLé•¿åº¦"),
            "enable_dns_check": ConfigField(type=bool, default=True, description="æ˜¯å¦è¿›è¡ŒDNSè§£ææ£€æŸ¥"),
            "dns_timeout": ConfigField(type=int, default=5, description="DNSè§£æè¶…æ—¶æ—¶é—´(ç§’)")
        }
    }
    plugin_instance = None

    def __init__(self, *args, **kwargs):
        UrlSummaryPlugin.plugin_instance = self
        super().__init__(*args, **kwargs)
        logger.info("URLæ‘˜è¦æ’ä»¶å·²åŠ è½½ - å®‰å…¨å¢å¼ºç‰ˆæœ¬")

    def get_help(self):
        return "ç”¨æ³•: /url_summary <ç½‘å€>\nåŠŸèƒ½: è·å–ç½‘é¡µå†…å®¹æ‘˜è¦ã€‚"

    def get_plugin_components(self) -> List[Tuple[ComponentInfo, Type]]:
        if not self.get_config("general.enabled", True):
            return []
        components = []
        components.append((UrlSummaryAction.get_action_info(), UrlSummaryAction))
        logger.info("URLæ‘˜è¦æ’ä»¶å·²åŠ è½½ï¼Œå®‰å…¨æ£€æŸ¥: %s", self.get_config("security.enable_security_check", True))
        return components
