import logging
import re
import aiohttp
import asyncio
import urllib.parse
from bs4 import BeautifulSoup
from typing import List, Tuple, Optional, Set, Type
from collections import OrderedDict
import time
from src.plugin_system import (
    BasePlugin, register_plugin, BaseAction,
    ComponentInfo, ActionActivationType, ConfigField, ChatMode
)
from src.plugin_system.apis import message_api, send_api, config_api, emoji_api

logger = logging.getLogger(__name__)

# --------- æœ¬è½®æ¿€æ´»URLå»é‡ ---------
_recently_activated_urls: Set[str] = set()

def should_skip_url_activation(url: str) -> bool:
    """
    å¦‚æœ URL åœ¨æœ¬èŠå¤©è¿›ç¨‹ä¸­å·²æ¿€æ´»è¿‡ä¸€æ¬¡ï¼Œåˆ™è·³è¿‡åç»­æ¿€æ´»ã€‚
    """
    if url in _recently_activated_urls:
        return True
    _recently_activated_urls.add(url)
    return False

try:
    from readability import Document
    readability_available = True
except ImportError:
    readability_available = False

# --------- æ¶ˆæ¯å»é‡ç¼“å­˜å®ç°ï¼ˆæ”¯æŒé…ç½®ï¼‰ ---------
recent_messages = OrderedDict()

def get_cache_ttl():
    try:
        plugin_inst = UrlSummaryPlugin.plugin_instance
        if plugin_inst:
            return plugin_inst.get_config("cache.cache_ttl", 600)
    except Exception:
        pass
    return 600

MAX_CACHE = 500

def is_duplicate_message(msg):
    now = time.time()
    cache_ttl = get_cache_ttl()
    if hasattr(msg, "id") and msg.id:
        key = f"id:{msg.id}"
    elif hasattr(msg, "plain_text") and msg.plain_text:
        key = f"hash:{hash(msg.plain_text)}"
    else:
        key = f"hash:{hash(str(msg))}"
    # æ¸…ç†è¿‡æœŸè®°å½•
    keys_to_del = [k for k, v in recent_messages.items() if now - v > cache_ttl]
    for k in keys_to_del:
        recent_messages.pop(k, None)
    if key in recent_messages:
        return True
    recent_messages[key] = now
    if len(recent_messages) > MAX_CACHE:
        recent_messages.popitem(last=False)
    return False

# --------- URLæ‘˜è¦ç¼“å­˜ ---------
url_summary_cache = OrderedDict()
MAX_URL_CACHE = 500

def get_url_cache_ttl():
    try:
        plugin_inst = UrlSummaryPlugin.plugin_instance
        if plugin_inst:
            return plugin_inst.get_config("cache.url_cache_ttl", 3600)
    except Exception:
        pass
    return 3600

def get_url_summary_from_cache(url):
    now = time.time()
    cache_ttl = get_url_cache_ttl()
    # æ¸…ç†è¿‡æœŸ
    keys_to_del = [k for k, v in url_summary_cache.items() if now - v['time'] > cache_ttl]
    for k in keys_to_del:
        url_summary_cache.pop(k, None)
    if url in url_summary_cache:
        url_summary_cache[url]['time'] = now
        return url_summary_cache[url]['summary']
    return None

def set_url_summary_cache(url, summary):
    now = time.time()
    url_summary_cache[url] = {'summary': summary, 'time': now}
    if len(url_summary_cache) > MAX_URL_CACHE:
        url_summary_cache.popitem(last=False)

class UrlSummaryAction(BaseAction):
    """ç½‘å€æ‘˜è¦Action - æ”¯æŒå…³é”®è¯å’ŒLLMåˆ¤æ–­ï¼Œé¿å…é‡å¤è§¦å‘"""
    action_name = "url_summary"
    action_description = "æ£€æµ‹æ¶ˆæ¯ä¸­çš„çœŸå®ç½‘å€å¹¶å‘é€å†…å®¹æ‘˜è¦"
    focus_activation_type = ActionActivationType.KEYWORD
    normal_activation_type = ActionActivationType.KEYWORD
    activation_keywords = ["http://", "https://", "www.", ".com", ".cn", ".net", ".org"]
    keyword_case_sensitive = False
    mode_enable = ChatMode.ALL
    parallel_action = False

    action_parameters = {"url": "è¦å¤„ç†çš„ç½‘é¡µURL"}
    action_require = [
        "ç”¨æˆ·æ¶ˆæ¯åŒ…å«æœ‰æ•ˆHTTP/HTTPSé“¾æ¥æ—¶ä½¿ç”¨",
        "é“¾æ¥é•¿åº¦å¤§äº7å­—ç¬¦ä¸”åŒ…å«åŸŸåæ—¶ä½¿ç”¨"
    ]
    llm_judge_prompt = "æ˜¯å¦éœ€è¦ç”Ÿæˆç½‘é¡µæ‘˜è¦ï¼Ÿæ¡ä»¶æ˜¯æ¶ˆæ¯åŒ…å«URLä¸”æœªé‡å¤ã€‚"
    associated_types = ["text"]

    DEFAULT_TIMEOUT = 10
    DEFAULT_MAX_LENGTH = 400
    MIN_URL_LENGTH = 7
    DEFAULT_MAX_SUBPAGE = 2
    DEFAULT_SUBPAGE_LENGTH = 200

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.url_validator = re.compile(
            r'^(?:(?:https?|ftp):\/\/)?'
            r'(?:\S+(?::\S*)?@)?'
            r'(?:(?:[a-zA-Z0-9\u00a1-\uffff-]{1,63}\.)+'
            r'[a-zA-Z\u00a1-\uffff]{2,})'
            r'(?::\d+)?'
            r'(?:[\/?#][^\s"]*)?'
            r'$', re.IGNORECASE
        )

    async def execute(self) -> Tuple[bool, str]:
        try:
            # æ¿€æ´»å‰æ£€æŸ¥ï¼šåŒè½®æ¬¡å·²å¤„ç†åˆ™è·³è¿‡
            urls = []
            if hasattr(self, 'action_data') and self.action_data.get("url"):
                urls = [self.normalize_url(self.action_data.get("url"))]
            elif hasattr(self, 'message') and self.message:
                urls = self.extract_and_validate_urls(self.message.plain_text)
            if urls and should_skip_url_activation(urls[0]):
                logger.info(f"URL å·²åœ¨æœ¬è½®æ¿€æ´»è¿‡ï¼Œè·³è¿‡æ‰§è¡Œ: {urls[0]}")
                return False, "è¯¥é“¾æ¥å·²å¤„ç†è¿‡"

            # æ¶ˆæ¯å±‚å»é‡
            if hasattr(self, 'message') and is_duplicate_message(self.message):
                logger.info("æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                return False, "å·²å¿½ç•¥é‡å¤æ¶ˆæ¯"

            # æå–URL
            urls = []
            if hasattr(self, 'action_data') and self.action_data.get("url"):
                urls = [self.normalize_url(self.action_data.get("url"))]
            elif hasattr(self, 'message') and self.message:
                urls = self.extract_and_validate_urls(self.message.plain_text)
            elif hasattr(self, 'raw_message') and self.raw_message:
                urls = self.extract_and_validate_urls(self.raw_message)
            if not urls:
                return False, "æœªæ£€æµ‹åˆ°æœ‰æ•ˆURL"
            url = urls[0]

            # æ£€æŸ¥æ‘˜è¦ç¼“å­˜
            cached = get_url_summary_from_cache(url)
            if cached:
                await self.send_summary(url, cached)
                return True, f"å·²å‘é€ {url} çš„ç¼“å­˜æ‘˜è¦"

            # é…ç½®
            timeout = self.get_config("http.timeout", self.DEFAULT_TIMEOUT)
            max_length = self.get_config("processing.max_length", self.DEFAULT_MAX_LENGTH)
            user_agent = self.get_config("http.user_agent", "Mozilla/5.0")
            max_subpage = self.get_config("processing.max_subpage", self.DEFAULT_MAX_SUBPAGE)
            subpage_length = self.get_config("processing.subpage_length", self.DEFAULT_SUBPAGE_LENGTH)
            enable_related = self.get_config("processing.enable_related_pages", True)

            await self.send_processing_feedback()
            seen = {url}
            summary = await self.get_url_summary(
                url, timeout, max_length, user_agent,
                fetch_links=enable_related, seen_links=seen,
                max_subpage=max_subpage, subpage_length=subpage_length
            )
            if summary:
                set_url_summary_cache(url, summary)
                await self.send_summary(url, summary)
                return True, f"å·²å‘é€ {url} çš„å†…å®¹æ‘˜è¦"
            return False, "æ— æ³•è·å–ç½‘é¡µå†…å®¹"
        except asyncio.TimeoutError:
            logger.warning("è¯·æ±‚è¶…æ—¶")
            await self.send_timeout_message()
            return False, "è¯·æ±‚è¶…æ—¶"
        except Exception as e:
            logger.exception("ç½‘å€æ‘˜è¦å¤„ç†å¤±è´¥")
            await self.send_error_message()
            return False, f"å¤„ç†å¤±è´¥: {str(e)}"

    async def send_processing_feedback(self):
        try:
            emoji_result = await emoji_api.get_by_emotion("processing")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€å¤„ç†è¡¨æƒ…å¤±è´¥: {str(e)}")

    async def send_summary(self, url: str, summary: str):
        display_url = url if len(url) <= 50 else f"{url[:30]}...{url[-20:]}"
        summary_msg = self.format_summary_message(display_url, summary)
        await self.send_text(summary_msg)
        try:
            emoji_result = await emoji_api.get_by_emotion("success")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€æˆåŠŸè¡¨æƒ…å¤±è´¥: {str(e)}")

    def format_summary_message(self, display_url: str, summary: str) -> str:
        parts = summary.split('\n\nç›¸å…³é¡µé¢ï¼š', 1)
        main = parts[0].strip()
        related = parts[1].strip() if len(parts) == 2 else None
        msg = f"ğŸ”— **ç½‘é¡µæ‘˜è¦** [`{display_url}`]\n\n> {main.replace(chr(10), '\n> ')}"
        if related:
            msg += "\n\n<details><summary>ç›¸å…³é¡µé¢</summary>\n\n"
            for sub in re.split(r"\nã€(https?://[^ã€‘]+)ã€‘\n", "\n"+related):
                if not sub.strip():
                    continue
                if sub.startswith("http"):
                    continue
                msg += f"> {sub.strip().replace(chr(10), '\\n> ')}\n"
            msg += "</details>"
        return msg

    async def send_timeout_message(self):
        try:
            await self.send_text("â±â±â± ç½‘é¡µåŠ è½½è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•")
            emoji_result = await emoji_api.get_by_emotion("timeout")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€è¶…æ—¶æ¶ˆæ¯å¤±è´¥: {str(e)}")

    async def send_error_message(self):
        try:
            await self.send_text("âŒâŒ å¤„ç†ç½‘é¡µå†…å®¹æ—¶å‡ºé”™ï¼Œè¯·ç¨åå†è¯•")
            emoji_result = await emoji_api.get_by_emotion("error")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {str(e)}")

    def extract_and_validate_urls(self, text: str) -> List[str]:
        url_pattern = r'((?:https?://)?(?:www\.)?(?:[a-zA-Z0-9\-]+\.)+[a-zA-Z]{2,}(?:/[^\s<>"]*)?)'
        potential_urls = re.findall(url_pattern, text)
        valid_urls = []
        for url in potential_urls:
            normalized_url = self.normalize_url(url)
            if self.is_valid_url(normalized_url):
                valid_urls.append(normalized_url)
        from collections import OrderedDict
        return list(OrderedDict.fromkeys(valid_urls))

    def normalize_url(self, url: str) -> str:
        url = urllib.parse.unquote(url.strip())
        if not url:
            return ""
        if not url.startswith(('http://', 'https://', 'ftp://')):
            url = "https://" + url.lstrip('/')
        return url

    def is_valid_url(self, url: str) -> bool:
        if len(url) < self.MIN_URL_LENGTH:
            return False
        if '.' not in url:
            return False
        return True

    async def get_url_summary(
        self,
        url: str,
        timeout: int,
        max_length: int,
        user_agent: str,
        fetch_links: bool = True,
        seen_links: Optional[Set[str]] = None,
        max_subpage: int = 2,
        subpage_length: int = 200
    ) -> Optional[str]:
        if not url.startswith(("http://", "https://")):
            return f"âš ï¸ æ— æ•ˆçš„URLæ ¼å¼: {url}"
        headers = {
            "User-Agent": user_agent,
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9"
        }
        proxy_url = self.get_config("http.proxy", "")
        if seen_links is None:
            seen_links = set()

        for attempt in range(3):
            try:
                async with aiohttp.ClientSession() as session:
                    logger.debug(f"å°è¯•è·å–URLå†…å®¹: {url} (å°è¯• {attempt+1}/3), ä»£ç†: {proxy_url}")
                    async with session.get(
                        url,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=timeout),
                        ssl=False,
                        proxy=proxy_url if proxy_url else None
                    ) as response:
                        if response.status != 200:
                            return f"âš ï¸ æ— æ³•è®¿é—®ç½‘é¡µ (çŠ¶æ€ç : {response.status})"
                        raw = await response.read()
                        encoding = response.charset
                        if not encoding:
                            try:
                                import chardet
                                encoding = chardet.detect(raw)['encoding']
                            except Exception:
                                encoding = 'utf-8'
                        try:
                            html = raw.decode(encoding or 'utf-8', errors='ignore')
                        except Exception:
                            html = raw.decode('utf-8', errors='ignore')

                        soup_for_links = BeautifulSoup(html, 'html.parser')
                        soup = BeautifulSoup(html, 'html.parser')
                        summary_mode = self.get_config("processing.summary_mode", "sentence")
                        content = self.extract_main_content(soup, html=html)
                        if summary_mode == "llm" and content:
                            summary = await self.summarize_by_llm(content, max_length)
                            meta_desc = soup.find("meta", attrs={"name": "description"}) or \
                                soup.find("meta", attrs={"property": "og:description"})
                            og_title = soup.find("meta", attrs={"property": "og:title"})
                            og_site = soup.find("meta", attrs={"property": "og:site_name"})
                            title = og_title.get("content", "").strip() if og_title else (soup.title.get_text(strip=True) if soup.title else "")
                            site = og_site.get("content", "").strip() if og_site else ""
                            desc = meta_desc.get("content", "").strip() if meta_desc else ""
                            lines = []
                            if title: lines.append(f"**{title}**")
                            if site: lines.append(f"ï¼ˆ{site}ï¼‰")
                            lines.append(summary)
                            summary = "\n".join(lines).strip()
                        else:
                            summary = self.extract_summary_from_soup(soup, html, max_length)
                        # ç›¸å…³é¡µé¢é€»è¾‘
                        if fetch_links:
                            internal_links = self.extract_internal_links(
                                soup_for_links, url, max_links=max_subpage, seen_links=seen_links
                            )
                            if internal_links:
                                for link in internal_links:
                                    seen_links.add(link)
                                related = await self.get_multi_url_summaries(
                                    internal_links, timeout, subpage_length, user_agent, seen_links=seen_links
                                )
                                if related:
                                    summary += "\n\nç›¸å…³é¡µé¢ï¼š"
                                    for link, sub_summary in related:
                                        link_disp = link if len(link) <= 50 else f"{link[:30]}...{link[-20:]}"
                                        summary += f"\nã€{link_disp}ã€‘\n{sub_summary}"
                        return summary
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/3): {type(e).__name__}")
                if attempt == 2:
                    return f"âŒâŒ è¯·æ±‚å¤±è´¥: {type(e).__name__}"
                await asyncio.sleep(1)
            except Exception as e:
                logger.exception("å¤„ç†é”™è¯¯")
                return f"âŒâŒ å¤„ç†é”™è¯¯: {type(e).__name__}"
        return "âŒâŒ å¤šæ¬¡å°è¯•åä»æ— æ³•è·å–å†…å®¹"

    async def get_multi_url_summaries(
        self, urls: list, timeout: int, max_length: int, user_agent: str, seen_links: Optional[Set[str]] = None
    ) -> list:
        results = []
        if seen_links is None:
            seen_links = set()
        for url in urls:
            if url in seen_links:
                continue
            cached_summary = get_url_summary_from_cache(url)
            if cached_summary:
                results.append((url, cached_summary))
                seen_links.add(url)
                continue
            try:
                summary = await self.get_url_summary(
                    url, timeout, max_length, user_agent, fetch_links=False, seen_links=seen_links
                )
                if summary:
                    set_url_summary_cache(url, summary)
                    results.append((url, summary))
                seen_links.add(url)
            except Exception as e:
                logger.warning(f"å­é¡µé¢æ‘˜è¦æŠ“å–å¤±è´¥: {url}, {str(e)}")
        return results

    def extract_internal_links(
        self, soup: BeautifulSoup, base_url: str, max_links: int = 2, seen_links: Optional[Set[str]] = None
    ) -> list:
        from urllib.parse import urljoin, urlparse
        base_domain = urlparse(base_url).netloc
        links = []
        seen = set() if seen_links is None else set(seen_links)
        for a in soup.find_all('a', href=True):
            href = a['href']
            abs_url = urljoin(base_url, href)
            link_domain = urlparse(abs_url).netloc
            if not abs_url.startswith(('http://', 'https://')):
                continue
            if (
                link_domain == base_domain
                and abs_url not in seen
                and abs_url != base_url
                and not abs_url.startswith('javascript:')
                and not abs_url.startswith('mailto:')
                and not abs_url.endswith(('.jpg', '.png', '.gif', '.svg', '.ico'))
            ):
                seen.add(abs_url)
                links.append(abs_url)
            if len(links) >= max_links:
                break
        return links

    def extract_main_content(self, soup: BeautifulSoup, html: str = None) -> str:
        if readability_available and html is not None:
            try:
                doc = Document(html)
                content = doc.summary()
                soup2 = BeautifulSoup(content, 'html.parser')
                text = soup2.get_text(" ", strip=True)
                if len(text) > 50:
                    logger.debug(f"readabilityæ­£æ–‡é•¿åº¦: {len(text)}")
                    return text
            except Exception as e:
                logger.warning(f"readabilityæŠ½å–æ­£æ–‡å¤±è´¥: {str(e)}")
        for tag in ['article', 'main', 'content', 'entry-content', 'body', 'section']:
            element = soup.find(tag)
            if element:
                text = element.get_text(" ", strip=True)
                logger.debug(f"æ ‡ç­¾<{tag}>æ­£æ–‡é•¿åº¦: {len(text)}")
                if len(text) > 50:
                    return text
        for class_name in [
            'wp_articlecontent', 'article-content', 'articleBody', 'article', 'main', 'content', 'entry-content',
            'body', 'post', 'post-content', 'main-content', 'TRS_Editor',
            'news-content', 'content-main', 'articleText', 'contentArea'
        ]:
            element = soup.find(class_=class_name)
            if element:
                text = element.get_text(" ", strip=True)
                logger.debug(f"class={class_name} æ­£æ–‡é•¿åº¦: {len(text)}")
                if len(text) > 50:
                    return text
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text(" ", strip=True)
            if len(text) > 10:
                paragraphs.append(text)
        if paragraphs:
            logger.debug(f"æŠ“åˆ°æ®µè½æ•°: {len(paragraphs)}ï¼Œåˆå¹¶å‰3æ®µè½ä¸ºï¼š{' | '.join(paragraphs[:3])}")
            return " ".join(paragraphs[:20])
        body = soup.body
        if body:
            text = body.get_text(" ", strip=True)
            logger.debug(f"<body>é•¿åº¦: {len(text)}")
            if len(text) > 50:
                return text
        a_tags = soup.find_all('a', href=True)
        headlines = []
        for a in a_tags:
            txt = a.get_text(strip=True)
            if txt and 5 < len(txt) < 40 and not re.search(r"[ã€Šã€‹]", txt):
                headlines.append(txt)
            if len(headlines) >= 5:
                break
        if headlines:
            logger.debug(f"headlines fallback: {' / '.join(headlines)}")
            return " / ".join(headlines)
        logger.debug("æ­£æ–‡æå–å…¨éƒ¨å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²")
        return ""

    def extract_main_content_html(self, html: str) -> Optional[str]:
        if readability_available and html is not None:
            try:
                doc = Document(html)
                return doc.summary(html_partial=True)
            except Exception as e:
                logger.warning(f"readabilityæ­£æ–‡htmlæŠ½å–å¤±è´¥: {str(e)}")
        return None

    def summarize_text(self, text: str, max_length: int = 400) -> str:
        import re
        text = re.sub(r'([a-zA-Z0-9])ã€‚([a-zA-Z0-9])', r'\1.\2', text)
        text = text.strip()
        if not text:
            return ""
        if len(text) <= max_length:
            return text

        summary_mode = "sentence"
        try:
            summary_mode = self.get_config("processing.summary_mode", "sentence")
        except Exception:
            pass

        if summary_mode == "llm":
            return "[LLMæ‘˜è¦å¤„ç†ä¸­...]"

        if summary_mode == "plain":
            trunc_point = -1
            for sep in ['ã€‚', 'ï¼', '!', 'ï¼Ÿ', '?', '.', '\n']:
                idx = text.rfind(sep, 0, max_length)
                if idx > trunc_point:
                    trunc_point = idx
            if trunc_point != -1 and trunc_point > max_length // 3:
                return text[:trunc_point + 1] + "..."
            return text[:max_length] + "..."

        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ!?\.])', text)
        result = ''
        total = 0
        for i in range(0, len(sentences) - 1, 2):
            seg = sentences[i] + sentences[i + 1]
            if total + len(seg) > max_length:
                break
            result += seg
            total += len(seg)
        if not result:
            return text[:max_length] + "..."
        return result.strip() + "..."

    async def summarize_by_llm(self, text, max_length: int) -> str:
        try:
            from src.plugin_system.apis import llm_api
            logger.info(f"[LLMæ‘˜è¦è°ƒç”¨] promptå‰100å­—: {text[:100]}")
            model_config_obj = llm_api.get_available_models()
            logger.info(f"modelsè·å–ç»“æœ: {model_config_obj}, ç±»å‹: {type(model_config_obj)}")
            model_config_key = self.get_config("processing.llm_config_key", "utils_small")
            model_config = getattr(model_config_obj, model_config_key, None)
            if not model_config:
                logger.warning(f"æœªæ‰¾åˆ°æŒ‡å®šæ¨¡å‹ {model_config_key}ï¼Œå°è¯• fallback")
                model_config = getattr(model_config_obj, "replyer_1", None)
            if not model_config:
                logger.error("æœªè·å–åˆ°ä»»ä½•å¯ç”¨æ¨¡å‹é…ç½®ï¼Œé™çº§æœ¬åœ°æ‘˜è¦")
                return self.summarize_text(text, max_length)
            prompt = f"è¯·å°†ä»¥ä¸‹å†…å®¹å‹ç¼©ä¸ºä¸è¶…è¿‡{max_length}å­—çš„ä¸­æ–‡æ‘˜è¦ï¼š\n{text}"
            success, response, reasoning, model_used = await llm_api.generate_with_model(prompt, model_config)
            logger.info(f"[LLMæ‘˜è¦è°ƒç”¨] model={model_used}, success={success}, responseå‰100å­—={response[:100] if response else response}")
            if success and response:
                return response.strip()
            else:
                logger.error("[LLMæ‘˜è¦è°ƒç”¨] LLMæœªè¿”å›ç»“æœï¼Œå›é€€æœ¬åœ°æ‘˜è¦")
                return self.summarize_text(text, max_length)
        except Exception as e:
            logger.exception(f"[LLMæ‘˜è¦è°ƒç”¨] å¤±è´¥: {e}")
            return self.summarize_text(text, max_length)       

    def extract_summary_from_soup(self, soup: BeautifulSoup, html: str, max_length: int) -> str:
        meta_desc = soup.find("meta", attrs={"name": "description"}) or \
            soup.find("meta", attrs={"property": "og:description"})
        og_title = soup.find("meta", attrs={"property": "og:title"})
        og_site = soup.find("meta", attrs={"property": "og:site_name"})
        title = og_title.get("content", "").strip() if og_title else (soup.title.get_text(strip=True) if soup.title else "")
        site = og_site.get("content", "").strip() if og_site else ""
        desc = meta_desc.get("content", "").strip() if meta_desc else ""
        content = self.extract_main_content(soup, html=html)
        summary_mode = "sentence"
        try:
            summary_mode = self.get_config("processing.summary_mode", "sentence")
            logger.info(f"[æ‘˜è¦æµç¨‹] summary_mode={summary_mode}")
        except Exception:
            pass
        logger.info(f"[æ‘˜è¦æµç¨‹] summary_mode={summary_mode}, contentå‰50å­—: {content[:50]}")
        if summary_mode == "llm" and content:
            summary = "[[LLMæ‘˜è¦å¤„ç†ä¸­]]"
        else:
            summary = desc if desc else self.summarize_text(content, max_length)
        lines = []
        if title: lines.append(f"**{title}**")
        if site: lines.append(f"ï¼ˆ{site}ï¼‰")
        lines.append(summary)
        return "\n".join(lines).strip()

@register_plugin
class UrlSummaryPlugin(BasePlugin):
    plugin_name = "url_summary_plugin"
    plugin_description = "è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„çœŸå®ç½‘å€å¹¶å‘é€å†…å®¹æ‘˜è¦ï¼ˆåŒ…æ‹¬é‡è¦å†…é“¾ï¼‰"
    plugin_version = "2.3.3"
    plugin_author = "qingkong"
    enable_plugin = True
    config_file_name = "config.toml"
    config_section_descriptions = {
        "general": "é€šç”¨è®¾ç½®",
        "http": "HTTPè¯·æ±‚è®¾ç½®",
        "processing": "å†…å®¹å¤„ç†è®¾ç½®",
        "cache": "ç¼“å­˜è®¾ç½®"
    }
    config_schema = {
            "config_version": ConfigField(type=str, default="1.0.0", description="é…ç½®ç‰ˆæœ¬"),
            "general": {
                "enabled": ConfigField(type=bool, default=True, description="æ˜¯å¦å¯ç”¨æ’ä»¶"),
                "enable_group": ConfigField(type=bool, default=True, description="æ˜¯å¦åœ¨ç¾¤èŠå¯ç”¨"),
                "enable_private": ConfigField(type=bool, default=True, description="æ˜¯å¦åœ¨ç§èŠå¯ç”¨")
            },
            "http": {
                "timeout": ConfigField(type=int, default=10, description="è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"),
                "user_agent": ConfigField(
                    type=str,
                    default="Mozilla/5.0 (compatible; MaiBot-URL-Summary/1.0)",
                    description="HTTPè¯·æ±‚ä½¿ç”¨çš„User-Agent"
                ),
                "max_retries": ConfigField(type=int, default=3, description="æœ€å¤§é‡è¯•æ¬¡æ•°"),
                "proxy": ConfigField(type=str, default="http://127.0.0.1:7890", description="HTTPè¯·æ±‚æ‰€ç”¨çš„ä»£ç†åœ°å€ï¼Œå¦‚ http://127.0.0.1:7890")
            },
            "processing": {
                "max_length": ConfigField(type=int, default=400, description="æ‘˜è¦æœ€å¤§é•¿åº¦"),
                "include_title": ConfigField(type=bool, default=True, description="æ˜¯å¦åŒ…å«æ ‡é¢˜"),
                "min_content_length": ConfigField(type=int, default=100, description="æœ€å°å†…å®¹é•¿åº¦"),
                "max_subpage": ConfigField(type=int, default=2, description="ç›¸å…³é¡µé¢æœ€å¤šæŠ“å–æ•°é‡"),
                "subpage_length": ConfigField(type=int, default=200, description="ç›¸å…³é¡µé¢æ‘˜è¦æœ€å¤§é•¿åº¦"),
                "enable_related_pages": ConfigField(type=bool, default=True, description="æ˜¯å¦æŠ“å–ç«™å†…ç›¸å…³é¡µé¢æ‘˜è¦"),
                "summary_mode": ConfigField(
                    type=str,
                    default="sentence",
                    description="æ‘˜è¦ç”Ÿæˆæ–¹å¼ï¼Œå¯é€‰ llmï¼ˆæ™ºèƒ½æ‘˜è¦ï¼‰ã€sentenceï¼ˆæŒ‰å¥æˆªæ–­ï¼‰ã€plainï¼ˆåŸæ ·æˆªæ–­ï¼‰"
                ),
                "llm_config_key": ConfigField(
                    type=str,
                    default="utils_small",
                    description="LLMæ‘˜è¦æ—¶é‡‡ç”¨çš„æ¨¡å‹é…ç½®keyï¼Œä¾‹å¦‚ï¼šutils_small, replyer_1, replyer_2"
                )
            },
            "cache": {
                "cache_ttl": ConfigField(type=int, default=600, description="é˜²é‡å¤ç¼“å­˜æ—¶é—´(ç§’)"),
                "url_cache_ttl": ConfigField(type=int, default=3600, description="URLæ‘˜è¦ç¼“å­˜æ—¶é—´(ç§’)")
            }
        }
    plugin_instance = None

    def __init__(self, *args, **kwargs):
        UrlSummaryPlugin.plugin_instance = self
        super().__init__(*args, **kwargs)

    def get_plugin_components(self) -> List[Tuple[ComponentInfo, Type]]:
        if not self.get_config("general.enabled", True):
            return []
        components = []
        components.append((UrlSummaryAction.get_action_info(), UrlSummaryAction))
        logger.info("URLæ‘˜è¦æ’ä»¶å·²åŠ è½½ï¼Œæ”¯æŒæ¶ˆæ¯å¯¹è±¡: %s", hasattr(UrlSummaryAction, 'message'))
        return components
