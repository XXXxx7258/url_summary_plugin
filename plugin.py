import logging
import re
import aiohttp
import asyncio
import urllib.parse
from bs4 import BeautifulSoup
from typing import List, Tuple, Type, Optional, Set
from collections import OrderedDict
import time
from src.plugin_system import (
    BasePlugin, register_plugin, BaseAction,
    ComponentInfo, ActionActivationType, ConfigField, ChatMode
)
from src.plugin_system.apis import message_api, send_api, config_api, emoji_api

logger = logging.getLogger(__name__)

try:
    from readability import Document
    readability_available = True
except ImportError:
    readability_available = False

# --------- å»é‡ç¼“å­˜å®ç°ï¼ˆæ”¯æŒé…ç½®ï¼‰ ---------
recent_messages = OrderedDict()

def get_cache_ttl():
    # åŠ¨æ€è¯»å–ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œæ— é…ç½®åˆ™ç”¨600
    try:
        plugin_inst = UrlSummaryPlugin.plugin_instance
        if plugin_inst:
            return plugin_inst.get_config("cache.cache_ttl", 600)
    except Exception:
        pass
    return 600

MAX_CACHE = 500

def is_duplicate_message(msg):
    now = time.time()
    cache_ttl = get_cache_ttl()
    key = None
    # ä¼˜å…ˆç”¨æ¶ˆæ¯IDï¼Œå¦‚æ— åˆ™ç”¨å†…å®¹hash
    if hasattr(msg, "id") and msg.id:
        key = f"id:{msg.id}"
    elif hasattr(msg, "plain_text") and msg.plain_text:
        key = f"hash:{hash(msg.plain_text)}"
    else:
        key = f"hash:{hash(str(msg))}"
    # æ¸…ç†è¿‡æœŸ
    keys_to_del = [k for k, v in recent_messages.items() if now - v > cache_ttl]
    for k in keys_to_del:
        recent_messages.pop(k, None)
    if key in recent_messages:
        return True
    recent_messages[key] = now
    if len(recent_messages) > MAX_CACHE:
        recent_messages.popitem(last=False)
    return False

class UrlSummaryAction(BaseAction):
    """ç½‘å€æ‘˜è¦Action - æ™ºèƒ½æ£€æµ‹å¹¶æ€»ç»“ç½‘é¡µå†…å®¹ï¼Œå¹¶å¯¹ä¸»ç«™å†…é‡è¦é“¾æ¥åšäºŒçº§æ‘˜è¦"""
    # === æ¿€æ´»æ§åˆ¶ ===
    action_name = "url_summary"
    action_description = "æ£€æµ‹æ¶ˆæ¯ä¸­çš„çœŸå®ç½‘å€å¹¶å‘é€å†…å®¹æ‘˜è¦"
    focus_activation_type = ActionActivationType.KEYWORD
    normal_activation_type = ActionActivationType.KEYWORD
    activation_keywords = ["http://", "https://", "www.", ".com", ".cn", ".net", ".org"]
    keyword_case_sensitive = False
    mode_enable = ChatMode.ALL
    parallel_action = False

    # === åŠŸèƒ½å®šä¹‰ ===
    action_parameters = {"url": "è¦å¤„ç†çš„ç½‘é¡µURL"}
    action_require = [
        "å½“æ¶ˆæ¯åŒ…å«çœŸå®æœ‰æ•ˆçš„ç½‘å€æ—¶ä½¿ç”¨",
        "éœ€è¦æå–ç½‘é¡µä¸»è¦å†…å®¹æ—¶ä½¿ç”¨",
        "ç”¨æˆ·åˆ†äº«çœŸå®é“¾æ¥æ—¶æä¾›æ‘˜è¦"
    ]
    associated_types = ["text"]

    # é…ç½®é»˜è®¤å€¼
    DEFAULT_TIMEOUT = 10
    DEFAULT_MAX_LENGTH = 400  # å»ºè®®400å­—ç¬¦ï¼Œçº¦200æ±‰å­—
    MIN_URL_LENGTH = 7  # æ›´å®½æ¾ï¼šåŸæ¥æ˜¯10ï¼Œæ”¾å®½ä¸º7
    DEFAULT_MAX_SUBPAGE = 2      # æœ€å¤šæŠ“å–2ä¸ªå†…é“¾æ‘˜è¦
    DEFAULT_SUBPAGE_LENGTH = 200 # å­é¡µé¢æ‘˜è¦æœ€å¤§é•¿åº¦

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # æ›´å®½æ¾çš„æ­£åˆ™ï¼šå…è®¸æ²¡æœ‰åè®®ï¼Œä»…éœ€åŒ…å«ç‚¹å’Œä¸¤æ®µ
        self.url_validator = re.compile(
            r'^(?:(?:https?|ftp):\/\/)?'
            r'(?:\S+(?::\S*)?@)?'
            r'(?:(?:[a-zA-Z0-9\u00a1-\uffff-]{1,63}\.)+'
            r'[a-zA-Z\u00a1-\uffff]{2,})'
            r'(?::\d+)?'
            r'(?:[/?#][^\s"]*)?'
            r'$', re.IGNORECASE
        )

    async def execute(self) -> Tuple[bool, str]:
        # --------- å»é‡åˆ¤æ–­ ---------
        if hasattr(self, 'message'):
            if is_duplicate_message(self.message):
                logger.info("æ£€æµ‹åˆ°é‡å¤æ¶ˆæ¯ï¼Œè·³è¿‡å¤„ç†")
                return False, "å·²å¿½ç•¥é‡å¤æ¶ˆæ¯"
        # --------- ä¸šåŠ¡é€»è¾‘ ---------
        try:
            logger.debug(f"UrlSummaryAction æ”¶åˆ° action_data: {getattr(self, 'action_data', {})}")
            logger.debug(f"æ¶ˆæ¯å¯¹è±¡å­˜åœ¨: {hasattr(self, 'message')}")
            urls = []
            if hasattr(self, 'action_data') and self.action_data:
                url_param = self.action_data.get("url", "")
                logger.debug(f"è§„åˆ’å™¨æä¾›çš„URLå‚æ•°: {url_param}")
                if url_param and self.is_valid_url(url_param):
                    urls = [self.normalize_url(url_param)]
                else:
                    urls = self.extract_and_validate_urls(url_param)
            if not urls and hasattr(self, 'message') and self.message:
                logger.debug("å°è¯•ä»æ¶ˆæ¯å¯¹è±¡ä¸­æå–URL")
                message_text = self.message.plain_text
                urls = self.extract_and_validate_urls(message_text)
            if not urls and hasattr(self, 'raw_message') and self.raw_message:
                logger.debug("å°è¯•ä»åŸå§‹æ¶ˆæ¯æ–‡æœ¬ä¸­æå–URL")
                urls = self.extract_and_validate_urls(self.raw_message)
            if not urls:
                logger.debug("æœªæ£€æµ‹åˆ°æœ‰æ•ˆURLï¼Œè·³è¿‡å¤„ç†")
                return False, "æœªæ£€æµ‹åˆ°æœ‰æ•ˆURL"
            url = urls[0]
            logger.info(f"å¼€å§‹å¤„ç†URL: {url}")

            timeout = self.get_config("http.timeout", self.DEFAULT_TIMEOUT)
            max_length = self.get_config("processing.max_length", self.DEFAULT_MAX_LENGTH)
            user_agent = self.get_config(
                "http.user_agent",
                "Mozilla/5.0 (compatible; MaiBot-URL-Summary/1.0)"
            )
            max_subpage = self.get_config("processing.max_subpage", self.DEFAULT_MAX_SUBPAGE)
            subpage_length = self.get_config("processing.subpage_length", self.DEFAULT_SUBPAGE_LENGTH)
            enable_related_pages = self.get_config("processing.enable_related_pages", True)

            await self.send_processing_feedback()
            seen_links = set([url])
            summary = await self.get_url_summary(
                url, timeout, max_length, user_agent,
                fetch_links=enable_related_pages,
                seen_links=seen_links,
                max_subpage=max_subpage,
                subpage_length=subpage_length,
            )
            if summary:
                await self.send_summary(url, summary)
                return True, f"å·²å‘é€ {url} çš„å†…å®¹æ‘˜è¦"
            return False, "æ— æ³•è·å–ç½‘é¡µå†…å®¹"
        except asyncio.TimeoutError:
            logger.warning("è¯·æ±‚è¶…æ—¶")
            await self.send_timeout_message()
            return False, "è¯·æ±‚è¶…æ—¶"
        except Exception as e:
            logger.exception("ç½‘å€æ‘˜è¦å¤„ç†å¤±è´¥")
            await self.send_error_message()
            return False, f"å¤„ç†å¤±è´¥: {str(e)}"

    async def send_processing_feedback(self):
        try:
            emoji_result = await emoji_api.get_by_emotion("processing")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€å¤„ç†è¡¨æƒ…å¤±è´¥: {str(e)}")

    async def send_summary(self, url: str, summary: str):
        display_url = url if len(url) <= 50 else f"{url[:30]}...{url[-20:]}"
        summary_msg = f"ğŸ”—ğŸ”— ç½‘é¡µæ‘˜è¦ [{display_url}]:\n{summary}"
        await self.send_text(summary_msg)
        try:
            emoji_result = await emoji_api.get_by_emotion("success")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€æˆåŠŸè¡¨æƒ…å¤±è´¥: {str(e)}")

    async def send_timeout_message(self):
        try:
            await self.send_text("â±â±â± ç½‘é¡µåŠ è½½è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•")
            emoji_result = await emoji_api.get_by_emotion("timeout")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€è¶…æ—¶æ¶ˆæ¯å¤±è´¥: {str(e)}")

    async def send_error_message(self):
        try:
            await self.send_text("âŒâŒ å¤„ç†ç½‘é¡µå†…å®¹æ—¶å‡ºé”™ï¼Œè¯·ç¨åå†è¯•")
            emoji_result = await emoji_api.get_by_emotion("error")
            if emoji_result:
                emoji_base64, _, _ = emoji_result
                await send_api.emoji_to_user(emoji_base64, self.user_id)
        except Exception as e:
            logger.warning(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {str(e)}")

    def extract_and_validate_urls(self, text: str) -> List[str]:
        url_pattern = r'((?:https?://)?(?:www\.)?(?:[a-zA-Z0-9\-]+\.)+[a-zA-Z]{2,}(?:/[^\s<>"]*)?)'
        potential_urls = re.findall(url_pattern, text)
        valid_urls = []
        for url in potential_urls:
            normalized_url = self.normalize_url(url)
            if self.is_valid_url(normalized_url):
                valid_urls.append(normalized_url)
        return valid_urls

    def normalize_url(self, url: str) -> str:
        url = urllib.parse.unquote(url.strip())
        if not url:
            return ""
        if not url.startswith(('http://', 'https://', 'ftp://')):
            url = "https://" + url.lstrip('/')
        return url

    def is_valid_url(self, url: str) -> bool:
        if len(url) < self.MIN_URL_LENGTH:
            return False
        if '.' not in url:
            return False
        return True

    async def get_url_summary(
        self,
        url: str,
        timeout: int,
        max_length: int,
        user_agent: str,
        fetch_links: bool = True,
        seen_links: Optional[Set[str]] = None,
        max_subpage: int = 2,
        subpage_length: int = 200
    ) -> Optional[str]:
        if not url.startswith(("http://", "https://")):
            return f"âš ï¸ æ— æ•ˆçš„URLæ ¼å¼: {url}"
        headers = {"User-Agent": user_agent}
        if seen_links is None:
            seen_links = set()
        for attempt in range(3):
            try:
                async with aiohttp.ClientSession() as session:
                    logger.debug(f"å°è¯•è·å–URLå†…å®¹: {url} (å°è¯• {attempt+1}/3)")
                    async with session.get(
                        url,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=timeout),
                        ssl=False
                    ) as response:
                        if response.status != 200:
                            return f"âš ï¸ æ— æ³•è®¿é—®ç½‘é¡µ (çŠ¶æ€ç : {response.status})"
                        raw = await response.read()
                        encoding = response.charset
                        if not encoding:
                            try:
                                import chardet
                                encoding = chardet.detect(raw)['encoding']
                            except Exception:
                                encoding = 'utf-8'
                        try:
                            html = raw.decode(encoding or 'utf-8', errors='ignore')
                        except Exception:
                            html = raw.decode('utf-8', errors='ignore')

                        soup_for_links = BeautifulSoup(html, 'html.parser')
                        soup = BeautifulSoup(html, 'html.parser')
                        summary = self.extract_summary_from_soup(soup, html, max_length)
                        if fetch_links:
                            internal_links = self.extract_internal_links(
                                soup_for_links, url, max_links=max_subpage, seen_links=seen_links
                            )
                            if internal_links:
                                for link in internal_links:
                                    seen_links.add(link)
                                related = await self.get_multi_url_summaries(
                                    internal_links, timeout, subpage_length, user_agent, seen_links=seen_links
                                )
                                if related:
                                    summary += "\n\nç›¸å…³é¡µé¢ï¼š"
                                    for link, sub_summary in related:
                                        link_disp = link if len(link) <= 50 else f"{link[:30]}...{link[-20:]}"
                                        summary += f"\nã€{link_disp}ã€‘\n{sub_summary}"
                        return summary
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt+1}/3): {type(e).__name__}")
                if attempt == 2:
                    return f"âŒâŒ è¯·æ±‚å¤±è´¥: {type(e).__name__}"
                await asyncio.sleep(1)
            except Exception as e:
                logger.exception("å¤„ç†é”™è¯¯")
                return f"âŒâŒ å¤„ç†é”™è¯¯: {type(e).__name__}"
        return "âŒâŒ å¤šæ¬¡å°è¯•åä»æ— æ³•è·å–å†…å®¹"

    async def get_multi_url_summaries(
        self, urls: list, timeout: int, max_length: int, user_agent: str, seen_links: Optional[Set[str]] = None
    ) -> list:
        results = []
        if seen_links is None:
            seen_links = set()
        for url in urls:
            if url in seen_links:
                continue
            try:
                summary = await self.get_url_summary(
                    url, timeout, max_length, user_agent, fetch_links=False, seen_links=seen_links
                )
                if summary:
                    results.append((url, summary))
                seen_links.add(url)
            except Exception as e:
                logger.warning(f"å­é¡µé¢æ‘˜è¦æŠ“å–å¤±è´¥: {url}, {str(e)}")
        return results

    def extract_internal_links(
        self, soup: BeautifulSoup, base_url: str, max_links: int = 2, seen_links: Optional[Set[str]] = None
    ) -> list:
        from urllib.parse import urljoin, urlparse
        base_domain = urlparse(base_url).netloc
        links = []
        seen = set() if seen_links is None else set(seen_links)
        for a in soup.find_all('a', href=True):
            href = a['href']
            abs_url = urljoin(base_url, href)
            link_domain = urlparse(abs_url).netloc
            if not abs_url.startswith(('http://', 'https://')):
                continue
            if (
                link_domain == base_domain
                and abs_url not in seen
                and abs_url != base_url
                and not abs_url.startswith('javascript:')
                and not abs_url.startswith('mailto:')
                and not abs_url.endswith('.jpg')
                and not abs_url.endswith('.png')
                and not abs_url.endswith('.gif')
                and not abs_url.endswith('.svg')
                and not abs_url.endswith('.ico')
            ):
                seen.add(abs_url)
                links.append(abs_url)
            if len(links) >= max_links:
                break
        return links

    def extract_summary_from_soup(self, soup: BeautifulSoup, html: str, max_length: int) -> str:
        meta_desc = soup.find("meta", attrs={"name": "description"}) or \
            soup.find("meta", attrs={"property": "og:description"})
        og_title = soup.find("meta", attrs={"property": "og:title"})
        og_site = soup.find("meta", attrs={"property": "og:site_name"})
        title = og_title.get("content", "").strip() if og_title else (soup.title.get_text(strip=True) if soup.title else "")
        site = og_site.get("content", "").strip() if og_site else ""
        desc = meta_desc.get("content", "").strip() if meta_desc else ""
        content = self.extract_main_content(soup, html=html)
        summary = desc if desc else self.summarize_text(content, max_length)
        lines = []
        if title: lines.append(f"[{title}]")
        if site: lines.append(f"ï¼ˆ{site}ï¼‰")
        lines.append(summary)
        return "\n".join(lines).strip()

    def extract_main_content(self, soup: BeautifulSoup, html: str = None) -> str:
        if readability_available and html is not None:
            try:
                doc = Document(html)
                content = doc.summary()
                soup2 = BeautifulSoup(content, 'html.parser')
                text = soup2.get_text(" ", strip=True)
                if len(text) > 50:
                    return text
            except Exception as e:
                logger.warning(f"readabilityæŠ½å–æ­£æ–‡å¤±è´¥: {str(e)}")
        for tag in ['article', 'main', 'content', 'entry-content']:
            element = soup.find(tag)
            if element:
                return element.get_text(" ", strip=True)
        for class_name in ['content', 'article', 'post-content', 'main-content', 'body']:
            element = soup.find(class_=class_name)
            if element:
                return element.get_text(" ", strip=True)
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text(" ", strip=True)
            if 30 < len(text) < 1500:
                paragraphs.append(text)
        if not paragraphs:
            a_tags = soup.find_all('a', href=True)
            headlines = []
            for a in a_tags:
                txt = a.get_text(strip=True)
                if txt and 5 < len(txt) < 40 and not re.search(r"[ã€Šã€‹]", txt):
                    headlines.append(txt)
                if len(headlines) >= 5:
                    break
            if headlines:
                return " / ".join(headlines)
        return " ".join(paragraphs[:15])

    def extract_main_content_html(self, html: str) -> Optional[str]:
        if readability_available and html is not None:
            try:
                doc = Document(html)
                return doc.summary(html_partial=True)
            except Exception as e:
                logger.warning(f"readabilityæ­£æ–‡htmlæŠ½å–å¤±è´¥: {str(e)}")
        return None

    def summarize_text(self, text: str, max_length: int=400) -> str:
        import re
        text = re.sub(r'([a-zA-Z0-9])ã€‚([a-zA-Z0-9])', r'\1.\2', text)
        sentences = re.split(r'([ã€‚.!ï¼?\n])', text)
        summary = ""
        for i in range(0, len(sentences), 2):
            s = sentences[i].strip()
            if not s:
                continue
            end = sentences[i+1] if i+1 < len(sentences) else ""
            if re.match(r'^https?://', s) or re.match(r'^www\.', s) or ('.' in s and ' ' not in s and not re.search(r'[\u4e00-\u9fa5]', s)):
                summary += s
            else:
                summary += s + (end if end else "ã€‚")
            if len(summary) > max_length:
                summary = summary[:max_length]
                break
        return summary[:max_length] + ("..." if len(summary) > max_length else "")

    def truncate_text(self, text: str, max_length: int) -> str:
        if len(text) <= max_length:
            return text
        trunc_point = max_length
        for i in range(max_length, max(0, max_length-100), -1):
            if text[i] in ('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', '\n'):
                trunc_point = i + 1
                break
        return text[:trunc_point] + "..."

@register_plugin
class UrlSummaryPlugin(BasePlugin):
    plugin_name = "url_summary_plugin"
    plugin_description = "è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯ä¸­çš„çœŸå®ç½‘å€å¹¶å‘é€å†…å®¹æ‘˜è¦ï¼ˆåŒ…æ‹¬é‡è¦å†…é“¾ï¼‰"
    plugin_version = "2.3.3"
    plugin_author = "Your Name"
    enable_plugin = True
    config_file_name = "config.toml"
    config_section_descriptions = {
        "general": "é€šç”¨è®¾ç½®",
        "http": "HTTPè¯·æ±‚è®¾ç½®",
        "processing": "å†…å®¹å¤„ç†è®¾ç½®",
        "cache": "ç¼“å­˜è®¾ç½®"
    }
    config_schema = {
        "general": {
            "enabled": ConfigField(type=bool, default=True, description="æ˜¯å¦å¯ç”¨æ’ä»¶"),
            "enable_group": ConfigField(type=bool, default=True, description="æ˜¯å¦åœ¨ç¾¤èŠå¯ç”¨"),
            "enable_private": ConfigField(type=bool, default=True, description="æ˜¯å¦åœ¨ç§èŠå¯ç”¨")
        },
        "http": {
            "timeout": ConfigField(type=int, default=10, description="è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)"),
            "user_agent": ConfigField(
                type=str,
                default="Mozilla/5.0 (compatible; MaiBot-URL-Summary/1.0)",
                description="HTTPè¯·æ±‚ä½¿ç”¨çš„User-Agent"
            ),
            "max_retries": ConfigField(type=int, default=3, description="æœ€å¤§é‡è¯•æ¬¡æ•°")
        },
        "processing": {
            "max_length": ConfigField(type=int, default=400, description="æ‘˜è¦æœ€å¤§é•¿åº¦"),
            "include_title": ConfigField(type=bool, default=True, description="æ˜¯å¦åŒ…å«æ ‡é¢˜"),
            "min_content_length": ConfigField(type=int, default=100, description="æœ€å°å†…å®¹é•¿åº¦"),
            "max_subpage": ConfigField(type=int, default=2, description="ç›¸å…³é¡µé¢æœ€å¤šæŠ“å–æ•°é‡"),
            "subpage_length": ConfigField(type=int, default=200, description="ç›¸å…³é¡µé¢æ‘˜è¦æœ€å¤§é•¿åº¦"),
            "enable_related_pages": ConfigField(type=bool, default=True, description="æ˜¯å¦æŠ“å–ç«™å†…ç›¸å…³é¡µé¢æ‘˜è¦")
        },
        "cache": {
            "cache_ttl": ConfigField(type=int, default=600, description="é˜²é‡å¤ç¼“å­˜æ—¶é—´(ç§’)")
        }
    }

    plugin_instance = None  # for global config reading

    def __init__(self, *args, **kwargs):
        UrlSummaryPlugin.plugin_instance = self
        super().__init__(*args, **kwargs)

    def get_plugin_components(self) -> List[Tuple[ComponentInfo, Type]]:
        if not self.get_config("general.enabled", True):
            return []
        components = []
        components.append((UrlSummaryAction.get_action_info(), UrlSummaryAction))
        logger.info("URLæ‘˜è¦æ’ä»¶å·²åŠ è½½ï¼Œæ”¯æŒæ¶ˆæ¯å¯¹è±¡: %s", hasattr(UrlSummaryAction, 'message'))
        return components